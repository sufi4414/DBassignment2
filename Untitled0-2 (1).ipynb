{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FopUdtSiFEDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Drive Input Path"
      ],
      "metadata": {
        "id": "MrCTklTLFGYG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMdrhwZFzkEl",
        "outputId": "d60a8028-3651-453c-9203-efef460477a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "46 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "input_path = \"/content/drive/MyDrive/student_files/data/TA_restaurants_curated_cleaned.csv\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vWU4ocm0eqA",
        "outputId": "b7f2fccc-2f17-4b86-f7f7-3c027f41685d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ],
      "metadata": {
        "id": "7dKt3_Ur0hRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# sc.stop() # uncomment this during debugging to restart your context in case execution stopped mid-way this cell.\n",
        "def show_schema(input_path):\n",
        "  try:\n",
        "    conf = SparkConf().setAppName(\"Part 1\")\n",
        "    sc = SparkContext(conf=conf)\n",
        "\n",
        "    spark = SparkSession(sc)\n",
        "\n",
        "    df = spark.read.option(\"header\", True).csv(input_path)\n",
        "    df.show()\n",
        "    df.printSchema()\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "show_schema(input_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Du1aREL0kJQ",
        "outputId": "371318e4-d8b2-4902-8eb1-c6291e053074"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+---------+--------------------+-------+------+-----------+-----------------+--------------------+--------------------+---------+\n",
            "|_c0|                Name|     City|       Cuisine Style|Ranking|Rating|Price Range|Number of Reviews|             Reviews|              URL_TA|    ID_TA|\n",
            "+---+--------------------+---------+--------------------+-------+------+-----------+-----------------+--------------------+--------------------+---------+\n",
            "|  0|Martine of Martin...|Amsterdam|[ 'French', 'Dutc...|    1.0|   5.0|   $$ - $$$|            136.0|[ [ 'Just like ho...|/Restaurant_Revie...|d11752080|\n",
            "|  1| De Silveren Spiegel|Amsterdam|[ 'Dutch', 'Europ...|    2.0|   4.5|       $$$$|            812.0|[ [ 'Great food a...|/Restaurant_Revie...|  d693419|\n",
            "|  2|             La Rive|Amsterdam|[ 'Mediterranean'...|    3.0|   4.5|       $$$$|            567.0|[ [ 'Satisfaction...|/Restaurant_Revie...|  d696959|\n",
            "|  3|            Vinkeles|Amsterdam|[ 'French', 'Euro...|    4.0|   5.0|       $$$$|            564.0|[ [ 'True five st...|/Restaurant_Revie...| d1239229|\n",
            "|  4|Librije's Zusje A...|Amsterdam|[ 'Dutch', 'Europ...|    5.0|   4.5|       $$$$|            316.0|[ [ 'Best meal......|/Restaurant_Revie...| d6864170|\n",
            "|  5|Ciel Bleu Restaurant|Amsterdam|[ 'Contemporary',...|    6.0|   4.5|       $$$$|            745.0|[ [ 'A treat!', '...|/Restaurant_Revie...|  d696902|\n",
            "|  6|              Zaza's|Amsterdam|[ 'French', 'Inte...|    7.0|   4.5|   $$ - $$$|           1455.0|[ [ '40th Birthda...|/Restaurant_Revie...| d1014732|\n",
            "|  7|Blue Pepper Resta...|Amsterdam|[ 'Asian', 'Indon...|    8.0|   4.5|       $$$$|            675.0|[ [ 'Great Experi...|/Restaurant_Revie...|  d697058|\n",
            "|  8|Teppanyaki Restau...|Amsterdam|[ 'Japanese', 'As...|    9.0|   4.5|       $$$$|            923.0|[ [ 'Great Food &...|/Restaurant_Revie...|  d697009|\n",
            "|  9|Rob Wigboldus Vis...|Amsterdam|[ 'Dutch', 'Seafo...|   10.0|   4.5|          $|            450.0|[ [ 'Excellent He...|/Restaurant_Revie...| d1955652|\n",
            "| 10|      The Happy Bull|Amsterdam|[ 'American', 'Ba...|   11.0|   4.5|   $$ - $$$|            295.0|[ [ 'Simply AMAZI...|/Restaurant_Revie...|d10275170|\n",
            "| 11|             Gartine|Amsterdam|[ 'French', 'Dutc...|   12.0|   4.5|   $$ - $$$|            967.0|[ [ 'A hidden gem...|/Restaurant_Revie...| d1014753|\n",
            "| 12|     Restaurant Adam|Amsterdam|[ 'French', 'Euro...|   13.0|   4.5|       $$$$|            368.0|[ [ 'Love it!', '...|/Restaurant_Revie...| d7695005|\n",
            "| 13|     Biercafe Gollem|Amsterdam|    [ 'Bar', 'Pub' ]|   14.0|   4.5|   $$ - $$$|            586.0|[ [ 'Awesome litt...|/Restaurant_Revie...| d3893242|\n",
            "| 14|  Restaurant Daalder|Amsterdam|[ 'French', 'Dutc...|   15.0|   4.5|       $$$$|           1246.0|[ [ 'Best meal of...|/Restaurant_Revie...| d1408533|\n",
            "| 15|Greenwoods Keizer...|Amsterdam|[ 'Dutch', 'Cafe'...|   16.0|   4.5|   $$ - $$$|           1391.0|[ [ 'So. Much. Fo...|/Restaurant_Revie...| d3200493|\n",
            "| 16|Omelegg - City Ce...|Amsterdam|[ 'Dutch', 'Europ...|   17.0|   4.5|          $|           1633.0|[ [ 'Brunch', 'Wo...|/Restaurant_Revie...| d8562698|\n",
            "| 17| Brasserie Ambassade|Amsterdam|[ 'French', 'Bar'...|   18.0|   4.5|       $$$$|            958.0|[ [ 'Wonderful Ch...|/Restaurant_Revie...| d8567150|\n",
            "| 18|   Sherpa Restaurant|Amsterdam|[ 'Indian', 'Tibe...|   19.0|   4.5|   $$ - $$$|            426.0|[ [ 'Very good ti...|/Restaurant_Revie...| d6022573|\n",
            "| 19|La Maschera Lillo...|Amsterdam|[ 'Italian', 'Med...|   20.0|   4.5|   $$ - $$$|            421.0|[ [ 'Fabulous Ita...|/Restaurant_Revie...|d10071792|\n",
            "+---+--------------------+---------+--------------------+-------+------+-----------+-----------------+--------------------+--------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Cuisine Style: string (nullable = true)\n",
            " |-- Ranking: string (nullable = true)\n",
            " |-- Rating: string (nullable = true)\n",
            " |-- Price Range: string (nullable = true)\n",
            " |-- Number of Reviews: string (nullable = true)\n",
            " |-- Reviews: string (nullable = true)\n",
            " |-- URL_TA: string (nullable = true)\n",
            " |-- ID_TA: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 1"
      ],
      "metadata": {
        "id": "alO2b_FR2daR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "def cleanup_csv(input_path):\n",
        "  try:\n",
        "    conf = SparkConf().setAppName(\"Part 1 Question 1\")\n",
        "    sc = SparkContext(conf=conf)\n",
        "    spark = SparkSession(sc)\n",
        "\n",
        "    df = spark.read.option(\"header\", True).csv(input_path)\n",
        "    print(f\"Original DataFrame count: {df.count()}\")\n",
        "\n",
        "    # Show the count of \"Reviews\" groups and their frequencies, then sort by count in descending order\n",
        "    df.groupBy(F.col(\"Reviews\")).agg(F.count(\"Reviews\").alias(\"count_Reviews\")).sort(F.desc(\"count_Reviews\")).show()\n",
        "\n",
        "    df_filtered = df.filter(\n",
        "          (df['Rating'].cast('float') >= 1.0) &\n",
        "          # (df['Reviews'] != \"[ [ ], [ ] ]\") & # do we consider this as empty or not empty?\n",
        "          (df['Reviews'].isNotNull())\n",
        "      )\n",
        "    print(f\"Filtered DataFrame count: {df_filtered.count()}\")\n",
        "    df_filtered.show()\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "# cleanup_csv(input_path)"
      ],
      "metadata": {
        "id": "EGxsXhDl2eiD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 2"
      ],
      "metadata": {
        "id": "4j5wJaJY69la"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "def worst_and_best_restaurant(input_path):\n",
        "\n",
        "  try:\n",
        "\n",
        "    conf = SparkConf().setAppName(\"Part 1 Question 2\")\n",
        "    sc = SparkContext(conf=conf)\n",
        "\n",
        "    spark = SparkSession(sc)\n",
        "\n",
        "    df = spark.read.option(\"header\", True).csv(input_path) # Replace with hdfs input_path\n",
        "\n",
        "    df = df.filter(df['Price Range'].isNotNull())\n",
        "    df = df.withColumn(\"Rating\", df[\"Rating\"].cast('float'))\n",
        "\n",
        "    # best_df = df.groupBy(\"City\", \"Price Range\").agg(F.max(\"Rating\").alias(\"Best_Rating\"))\n",
        "    # worst_df = df.groupBy(\"City\", \"Price Range\").agg(F.min(\"Rating\").alias(\"Worst_Rating\"))\n",
        "    best_df = (\n",
        "      df\n",
        "      .groupBy(\"City\", \"Price Range\")\n",
        "      .agg(F.max(\"Rating\"))\n",
        "      .withColumn(\"Rating\", F.col(\"max(Rating)\"))\n",
        "      .orderBy(\"City\")  # Sort by 'City' in ascending order\n",
        "    )\n",
        "\n",
        "    worst_df = (\n",
        "        df\n",
        "        .groupBy(\"City\", \"Price Range\")\n",
        "        .agg(F.min(\"Rating\"))\n",
        "        .withColumn(\"Rating\", F.col(\"min(Rating)\"))\n",
        "        .orderBy(\"City\")  # Sort by 'City' in ascending order\n",
        "    )\n",
        "    # print(\"Best restaurants....\")\n",
        "    # best_df.show()\n",
        "    # print(\"Worst restaurants....\")\n",
        "    # worst_df.show()\n",
        "\n",
        "    union_df = best_df.union(worst_df)\n",
        "    # print(\"Union df...\")\n",
        "    # union_df.show()\n",
        "\n",
        "    combined_df = union_df.join(df, on=[\"City\", \"Price Range\", \"Rating\"], how=\"inner\")\n",
        "    combined_df = (\n",
        "    combined_df.dropDuplicates([\"Price Range\", \"City\", \"Rating\"])\n",
        "    .select(\n",
        "        \"_c0\",\n",
        "        \"Name\",\n",
        "        \"City\",\n",
        "        \"Cuisine Style\",\n",
        "        \"Ranking\",\n",
        "        \"Rating\",\n",
        "        \"Price Range\",\n",
        "        \"Number of Reviews\",\n",
        "        \"Reviews\",\n",
        "        \"URL_TA\",\n",
        "        \"ID_TA\",\n",
        "    )\n",
        "    .sort(F.col(\"City\").asc(), F.col(\"Price Range\").asc(), F.col(\"Rating\").desc())\n",
        "  )\n",
        "\n",
        "    combined_df.show()\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "\n",
        "\n",
        "worst_and_best_restaurant(input_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H8uIXQt5B6_",
        "outputId": "200a5f9e-44c4-4af1-d86d-b036cf007ec1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+---------+--------------------+-------+------+-----------+-----------------+--------------------+--------------------+---------+\n",
            "| _c0|                Name|     City|       Cuisine Style|Ranking|Rating|Price Range|Number of Reviews|             Reviews|              URL_TA|    ID_TA|\n",
            "+----+--------------------+---------+--------------------+-------+------+-----------+-----------------+--------------------+--------------------+---------+\n",
            "| 163|          Sir Hummus|Amsterdam|[ 'Healthy', 'Mid...|  164.0|   5.0|          $|            126.0|[ [ 'Great servic...|/Restaurant_Revie...| d7607446|\n",
            "|2932|     Grillroom Sabba|Amsterdam|[ 'Middle Eastern' ]| 2942.0|   2.5|          $|             12.0|[ [ 'This is a gr...|/Restaurant_Revie...| d6464568|\n",
            "|   0|Martine of Martin...|Amsterdam|[ 'French', 'Dutc...|    1.0|   5.0|   $$ - $$$|            136.0|[ [ 'Just like ho...|/Restaurant_Revie...|d11752080|\n",
            "|3239|       Reggae Rita's|Amsterdam|[ 'Caribbean', 'J...|   NULL|  -1.0|   $$ - $$$|             NULL|[ [ 'A TRUE BLESS...|/Restaurant_Revie...|d12291891|\n",
            "|   3|            Vinkeles|Amsterdam|[ 'French', 'Euro...|    4.0|   5.0|       $$$$|            564.0|[ [ 'True five st...|/Restaurant_Revie...| d1239229|\n",
            "|3201|Maurya Indian Res...|Amsterdam|[ 'Indian', 'Asia...| 3212.0|   2.0|       $$$$|             51.0|[ [ 'Caution: exp...|/Restaurant_Revie...|  d696972|\n",
            "|   1|Cinque Wine & Del...|   Athens|[ 'Wine Bar', 'Gr...|    2.0|   5.0|          $|           1013.0|[ [ 'One of the b...|/Restaurant_Revie...| d9462540|\n",
            "|1747|         Crepa-Crepa|   Athens| [ 'Mediterranean' ]| 1749.0|   2.5|          $|              2.0|[ [ 'Order-in thu...|/Restaurant_Revie...|d12118518|\n",
            "|   0|Simul Gastronomic...|   Athens|[ 'European', 'Ve...|    1.0|   5.0|   $$ - $$$|            127.0|[ [ 'Excellent ex...|/Restaurant_Revie...| d9763282|\n",
            "|1749|            Seirines|   Athens|[ 'Greek', 'Medit...| 1751.0|   2.0|   $$ - $$$|             75.0|[ [ 'Quick lunch'...|/Restaurant_Revie...| d3708996|\n",
            "|   2|   Dinner in the Sky|   Athens|[ 'Mediterranean'...|    3.0|   5.0|       $$$$|            886.0|[ [ 'Great sensor...|/Restaurant_Revie...| d7825999|\n",
            "|1700|            El Greco|   Athens|         [ 'Greek' ]| 1702.0|   2.0|       $$$$|             27.0|[ [ 'Lovely food ...|/Restaurant_Revie...| d3862721|\n",
            "| 186|      Comida De Olla|Barcelona|[ 'Mediterranean'...|  187.0|   5.0|          $|             88.0|[ [ 'Must try', '...|/Restaurant_Revie...|d10325973|\n",
            "|7783|La Petite Champag...|Barcelona|[ 'International'...|   NULL|  -1.0|          $|             NULL|      [ [  ], [  ] ]|/Restaurant_Revie...|d13173275|\n",
            "|   1|               Viana|Barcelona|[ 'International'...|    2.0|   5.0|   $$ - $$$|           2707.0|[ [ 'Wow! Best ev...|/Restaurant_Revie...| d8531409|\n",
            "|7784|Les Dues Sicilies...|Barcelona|       [ 'Italian' ]|   NULL|  -1.0|   $$ - $$$|             NULL|      [ [  ], [  ] ]|/Restaurant_Revie...|d13199631|\n",
            "|   0|                 Uma|Barcelona|[ 'International'...|    1.0|   5.0|       $$$$|            792.0|[ [ 'Perfect atmo...|/Restaurant_Revie...| d8003030|\n",
            "|7773| White Shisha Lounge|Barcelona|  [ 'Delicatessen' ]| 7787.0|   1.0|       $$$$|             NULL|[ [ 'Tourist exto...|/Restaurant_Revie...|d12457377|\n",
            "| 107|        Naveena Path|   Berlin|[ 'Indian', 'Asia...|  108.0|   5.0|          $|            423.0|[ [ 'Mindblowing!...|/Restaurant_Revie...| d1963405|\n",
            "|6362|       Cafe MokannTi|   Berlin|[ 'German', 'Afri...|   NULL|  -1.0|          $|             NULL|      [ [  ], [  ] ]|/Restaurant_Revie...|d12249169|\n",
            "+----+--------------------+---------+--------------------+-------+------+-----------+-----------------+--------------------+--------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 3"
      ],
      "metadata": {
        "id": "-ltpRdEzMJ1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "def average_rating(input_path):\n",
        "  try:\n",
        "    conf = SparkConf().setAppName(\"Part 1 Question 3\")\n",
        "    sc = SparkContext(conf=conf)\n",
        "    spark = SparkSession(sc)\n",
        "\n",
        "    df = spark.read.option(\"header\", True).csv(input_path)\n",
        "    df = df.withColumn(\"Rating\", df[\"Rating\"].cast('float'))\n",
        "    avg_df = (\n",
        "        df\n",
        "        .groupBy(\"City\")\n",
        "        .agg(F.avg(\"Rating\").alias(\"AverageRating\"))\n",
        "        .orderBy(\"City\")  # Sort by 'City' in ascending order\n",
        "    )\n",
        "    # avg_df.show()\n",
        "\n",
        "    top3_df = (\n",
        "        avg_df\n",
        "        .orderBy(F.desc(\"AverageRating\"))\n",
        "        .limit(3)\n",
        "\n",
        "    )\n",
        "    # top3_df.show()\n",
        "\n",
        "    bot3_df = (\n",
        "        avg_df\n",
        "        .orderBy(\"AverageRating\")\n",
        "        .limit(3)\n",
        "\n",
        "    )\n",
        "    # bot3_df.show()\n",
        "\n",
        "    union_df = top3_df.union(bot3_df)\n",
        "    union_df.show()\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "average_rating(input_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZujRvAeGB2rA",
        "outputId": "51e1289e-40c6-4976-d787-b431ed2aad4c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------------+\n",
            "|     City|     AverageRating|\n",
            "+---------+------------------+\n",
            "|   Athens| 4.241316931982634|\n",
            "|     Rome|  4.19908330011957|\n",
            "|   Oporto| 4.164118705035971|\n",
            "|   Madrid|3.8445586975149957|\n",
            "|    Milan| 3.846076458752515|\n",
            "|Stockholm|3.8947530864197533|\n",
            "+---------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 4"
      ],
      "metadata": {
        "id": "grYgAWV0Stqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "def count_restaurant(input_path):\n",
        "  try:\n",
        "    conf = SparkConf().setAppName(\"Part 1 Question 4\")\n",
        "    sc = SparkContext(conf=conf)\n",
        "    spark = SparkSession(sc)\n",
        "\n",
        "    df = spark.read.option(\"header\", True).csv(input_path)\n",
        "\n",
        "\n",
        "    df = df.withColumn(\"Cuisine Style\", F.regexp_replace(\"Cuisine Style\", \"^\\\\[|\\\\]$\", \"\"))\n",
        "    df = df.withColumn(\"Cuisine Style\", F.split(F.col(\"Cuisine Style\"), \",\\s*\"))\n",
        "\n",
        "\n",
        "    df_exploded = df.withColumn(\"Cuisine\", F.explode(\"Cuisine Style\"))\n",
        "    df_exploded = df_exploded.withColumn(\"Cuisine\", F.regexp_replace(\"Cuisine\", \"'\", \"\"))\n",
        "    df_exploded = df_exploded.withColumn(\"Cuisine\", F.trim(\"Cuisine\"))\n",
        "\n",
        "\n",
        "    result_df = (\n",
        "        df_exploded.groupBy(\"City\", \"Cuisine\")\n",
        "        .count()\n",
        "        .orderBy(\"City\", F.col(\"count\").desc())\n",
        "    )\n",
        "\n",
        "\n",
        "    result_df = result_df.select(\n",
        "        F.col(\"City\").alias(\"City\"),\n",
        "        F.col(\"Cuisine\").alias(\"Cuisine\"),\n",
        "        F.col(\"count\")\n",
        "    )\n",
        "\n",
        "    result_df.show()\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "count_restaurant(input_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7achoI7dPa-D",
        "outputId": "f1b8529f-19e2-4a42-ba47-451b79c5005a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------+-----+\n",
            "|     City|            Cuisine|count|\n",
            "+---------+-------------------+-----+\n",
            "|Amsterdam|           European| 1418|\n",
            "|Amsterdam|Vegetarian Friendly| 1307|\n",
            "|Amsterdam|              Dutch|  805|\n",
            "|Amsterdam|      Vegan Options|  524|\n",
            "|Amsterdam|Gluten Free Options|  497|\n",
            "|Amsterdam|                Bar|  497|\n",
            "|Amsterdam|            Italian|  364|\n",
            "|Amsterdam|              Asian|  352|\n",
            "|Amsterdam|      International|  345|\n",
            "|Amsterdam|      Mediterranean|  337|\n",
            "|Amsterdam|               Cafe|  317|\n",
            "|Amsterdam|                Pub|  292|\n",
            "|Amsterdam|             French|  204|\n",
            "|Amsterdam|              Pizza|  182|\n",
            "|Amsterdam|           American|  142|\n",
            "|Amsterdam|            Seafood|  125|\n",
            "|Amsterdam|          Fast Food|  117|\n",
            "|Amsterdam|           Japanese|  113|\n",
            "|Amsterdam|         Steakhouse|  112|\n",
            "|Amsterdam|            Chinese|   92|\n",
            "+---------+-------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BPOb8GcIWjMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 2"
      ],
      "metadata": {
        "id": "lnxLET9_XihR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_path2 = \"/content/drive/MyDrive/student_files/data/tmdb_5000_credits.parquet\""
      ],
      "metadata": {
        "id": "cy2LsIHBYnDF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_schema_parquet(input_path):\n",
        "  try:\n",
        "    conf = SparkConf().setAppName(\"Part 1\")\n",
        "    sc = SparkContext(conf=conf)\n",
        "\n",
        "    spark = SparkSession(sc)\n",
        "\n",
        "    df = spark.read.option(\"header\", True).parquet(input_path)\n",
        "    df.show()\n",
        "    df.printSchema()\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "show_schema_parquet(input_path2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgXUvgTqZ1jU",
        "outputId": "f4ce188d-39d4-4888-f737-4d53c93bda30"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+--------------------+--------------------+\n",
            "|movie_id|               title|                cast|                crew|\n",
            "+--------+--------------------+--------------------+--------------------+\n",
            "|   19995|              Avatar|[{\"cast_id\": 242,...|[{\"credit_id\": \"5...|\n",
            "|     285|Pirates of the Ca...|[{\"cast_id\": 4, \"...|[{\"credit_id\": \"5...|\n",
            "|  206647|             Spectre|[{\"cast_id\": 1, \"...|[{\"credit_id\": \"5...|\n",
            "|   49026|The Dark Knight R...|[{\"cast_id\": 2, \"...|[{\"credit_id\": \"5...|\n",
            "|   49529|         John Carter|[{\"cast_id\": 5, \"...|[{\"credit_id\": \"5...|\n",
            "|     559|        Spider-Man 3|[{\"cast_id\": 30, ...|[{\"credit_id\": \"5...|\n",
            "|   38757|             Tangled|[{\"cast_id\": 34, ...|[{\"credit_id\": \"5...|\n",
            "|   99861|Avengers: Age of ...|[{\"cast_id\": 76, ...|[{\"credit_id\": \"5...|\n",
            "|     767|Harry Potter and ...|[{\"cast_id\": 3, \"...|[{\"credit_id\": \"5...|\n",
            "|  209112|Batman v Superman...|[{\"cast_id\": 18, ...|[{\"credit_id\": \"5...|\n",
            "|    1452|    Superman Returns|[{\"cast_id\": 3, \"...|[{\"credit_id\": \"5...|\n",
            "|   10764|   Quantum of Solace|[{\"cast_id\": 1, \"...|[{\"credit_id\": \"5...|\n",
            "|      58|Pirates of the Ca...|[{\"cast_id\": 37, ...|[{\"credit_id\": \"5...|\n",
            "|   57201|     The Lone Ranger|[{\"cast_id\": 4, \"...|[{\"credit_id\": \"5...|\n",
            "|   49521|        Man of Steel|[{\"cast_id\": 2, \"...|[{\"credit_id\": \"5...|\n",
            "|    2454|The Chronicles of...|[{\"cast_id\": 1, \"...|[{\"credit_id\": \"5...|\n",
            "|   24428|        The Avengers|[{\"cast_id\": 46, ...|[{\"credit_id\": \"5...|\n",
            "|    1865|Pirates of the Ca...|[{\"cast_id\": 15, ...|[{\"credit_id\": \"5...|\n",
            "|   41154|      Men in Black 3|[{\"cast_id\": 4, \"...|[{\"credit_id\": \"5...|\n",
            "|  122917|The Hobbit: The B...|[{\"cast_id\": 10, ...|[{\"credit_id\": \"5...|\n",
            "+--------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- movie_id: long (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- cast: string (nullable = true)\n",
            " |-- crew: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 5"
      ],
      "metadata": {
        "id": "NYZjmgVoXp1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql.types import ArrayType, StringType, StructField, StructType\n",
        "from pyspark.sql.functions import from_json, col, explode, array, array_sort, count\n",
        "import itertools\n",
        "\n",
        "def find_pairs(input_path):\n",
        "  try:\n",
        "    conf = SparkConf().setAppName(\"Part 1 Question 4\")\n",
        "    sc = SparkContext(conf=conf)\n",
        "    spark = SparkSession(sc)\n",
        "\n",
        "    json_schema = ArrayType(StructType([\n",
        "        StructField(\"name\", StringType(), nullable=False)\n",
        "    ]))\n",
        "\n",
        "    df = spark.read.option(\"header\", True).parquet(input_path)\n",
        "\n",
        "    df = df.drop(\"crew\")\n",
        "    actor1_df = df.withColumn(\"actor1\", F.explode(F.from_json(F.col(\"cast\"), json_schema)))\n",
        "    actor2_df = df.withColumn(\"actor2\", F.explode(F.from_json(F.col(\"cast\"), json_schema)))\n",
        "    actor1_df = actor1_df.select(\"movie_id\", \"title\", F.col(\"actor1.name\").alias(\"actor1\"))\n",
        "    actor2_df = actor2_df.select(\"movie_id\", \"title\", F.col(\"actor2.name\").alias(\"actor2\"))\n",
        "    # actor1_df.show()\n",
        "    # actor2_df.show()\n",
        "\n",
        "    paired_actors_df = actor1_df.alias(\"df1\") \\\n",
        "    .join(\n",
        "        actor2_df.alias(\"df2\"),\n",
        "        (col(\"df1.movie_id\") == col(\"df2.movie_id\")) &\n",
        "        (col(\"df1.actor1\") != col(\"df2.actor2\"))\n",
        "    ) \\\n",
        "    .select(\n",
        "        col(\"df1.movie_id\"),\n",
        "        col(\"df1.title\"),\n",
        "        col(\"df1.actor1\").alias(\"actor1\"),\n",
        "        col(\"df2.actor2\").alias(\"actor2\")\n",
        "    ) \\\n",
        "    .distinct() \\\n",
        "    .orderBy(\"movie_id\", \"actor1\", \"actor2\")\n",
        "\n",
        "    # paired_actors_df.show()\n",
        "    df_counts = paired_actors_df.groupBy(\"actor1\", \"actor2\") \\\n",
        "    .agg(count(\"*\").alias(\"pair_count\")) \\\n",
        "    .filter(col(\"pair_count\") >= 2) \\\n",
        "    .orderBy(\"actor1\", \"actor2\")\n",
        "\n",
        "    valid_pairs = df_counts.select(\"actor1\", \"actor2\")\n",
        "\n",
        "    # Join back to get all occurrences of these pairs along with movie details\n",
        "    final_df = paired_actors_df.join(valid_pairs, [\"actor1\", \"actor2\"], \"inner\")\n",
        "    final_df = final_df.select(\"movie_id\", \"title\", \"actor1\", \"actor2\")\n",
        "    final_df.show()\n",
        "\n",
        "  finally:\n",
        "    sc.stop()\n",
        "\n",
        "find_pairs(input_path2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WheYcvOkXlZJ",
        "outputId": "f85f0a41-6341-424d-ea1d-b3f91d68ce26"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+--------------+--------------------+\n",
            "|movie_id|               title|        actor1|              actor2|\n",
            "+--------+--------------------+--------------+--------------------+\n",
            "|  241254|          The Prince|       50 Cent|         John Cusack|\n",
            "|  199373|   The Frozen Ground|       50 Cent|         John Cusack|\n",
            "|   22821|The Boondock Sain...|A. Frank Ruffo|           Joe Parro|\n",
            "|    2976|           Hairspray|A. Frank Ruffo|           Joe Parro|\n",
            "|  114150|       Pitch Perfect| Aakomon Jones|          C.J. Perry|\n",
            "|  254470|     Pitch Perfect 2| Aakomon Jones|          C.J. Perry|\n",
            "|  114150|       Pitch Perfect| Aakomon Jones|      Donald Watkins|\n",
            "|  239566|           Get on Up| Aakomon Jones|      Donald Watkins|\n",
            "|  254470|     Pitch Perfect 2| Aakomon Jones|     Elizabeth Banks|\n",
            "|  114150|       Pitch Perfect| Aakomon Jones|     Elizabeth Banks|\n",
            "|  114150|       Pitch Perfect| Aakomon Jones|John Benjamin Hickey|\n",
            "|  239566|           Get on Up| Aakomon Jones|John Benjamin Hickey|\n",
            "|  114150|       Pitch Perfect| Aakomon Jones|John Michael Higgins|\n",
            "|  254470|     Pitch Perfect 2| Aakomon Jones|John Michael Higgins|\n",
            "|  254470|     Pitch Perfect 2| Aakomon Jones|      Shelley Regner|\n",
            "|  114150|       Pitch Perfect| Aakomon Jones|      Shelley Regner|\n",
            "|     921|      Cinderella Man|  Aaron Abrams|           Gene Pyrz|\n",
            "|    1577|Resident Evil: Ap...|  Aaron Abrams|           Gene Pyrz|\n",
            "|     314|            Catwoman| Aaron Douglas|       Alex Borstein|\n",
            "|   18736|The Lizzie McGuir...| Aaron Douglas|       Alex Borstein|\n",
            "+--------+--------------------+--------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vm3cnjzcfYaE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}